{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing stopwords in Sentence 1:\n",
      "['regret', 'life', 'drink', 'wine', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 영어의 stopwords를 불러와 변수에 저장한다.\n",
    "\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "# 전처리하고자 하는 문장을 String 변수로 저장한다.\n",
    "\n",
    "sent1 = 'My only regret in life is that I did not drink more wine.'\n",
    "\n",
    "sent2 = 'I drink to make other people more interesting.'\n",
    "\n",
    "sent3 = 'An intelligent man is sometimes forced to be drunk to spend time with his fools.'\n",
    "\n",
    "# 문장 1의 stopwords 제거\n",
    "\n",
    "print('Removing stopwords in Sentence 1:')\n",
    "\n",
    "result1 = []                                # stopwords가 제거된 결과를 담기위한 리스트를 생성한다.\n",
    "\n",
    "tokens1 = nltk.word_tokenize(sent1)         # 문장을 tokenize 한다.\n",
    "\n",
    "for token in tokens1:                       # for 문을 통해 각각의 token이 stopwords 인지 아닌지를 판별해 결과를 token에 저장한다.\n",
    "    \n",
    "    if token.lower() not in stopWords:      # 만약 소문자로 변환한 token이 stopWords 내에 없으면:\n",
    "        \n",
    "        result1.append(token)               # token을 리스트에 첨부한다.\n",
    "        \n",
    "print(result1)                              # 결과를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing stopwords in Sentence 2:\n",
      "['drink', 'make', 'people', 'interesting', '.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 2의 stopwords 제거\n",
    "\n",
    "print('Removing stopwords in Sentence 2:')\n",
    "\n",
    "result2 = []\n",
    "\n",
    "tokens2 = nltk.word_tokenize(sent2)\n",
    "\n",
    "for token in tokens2:\n",
    "    \n",
    "    if token.lower() not in stopWords:\n",
    "        \n",
    "        result2.append(token)\n",
    "        \n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing stopwords in Sentence 3:\n",
      "['intelligent', 'man', 'sometimes', 'forced', 'drunk', 'spend', 'time', 'fools', '.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 3의 stopwords 제거\n",
    "\n",
    "print('Removing stopwords in Sentence 3:')\n",
    "\n",
    "result3 = []\n",
    "\n",
    "token3 = nltk.word_tokenize(sent3)\n",
    "\n",
    "for token in token3:\n",
    "    \n",
    "    if token.lower() not in stopWords:\n",
    "        \n",
    "        result3.append(token)\n",
    "        \n",
    "print(result3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
